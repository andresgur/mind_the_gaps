{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14735cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mind_the_gaps.lightcurves import GappyLightcurve\n",
    "from mind_the_gaps.gpmodelling import GPModelling\n",
    "from mind_the_gaps.models.psd_models import BendingPowerlaw, Lorentzian, SHO, Matern32, Jitter\n",
    "from mind_the_gaps.models.celerite_models import Lorentzian as Lor\n",
    "from mind_the_gaps.models.celerite_models import DampedRandomWalk\n",
    "from mind_the_gaps.simulator import Simulator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import celerite, corner\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "cpus = 15 # set the number of cores for parallelization\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038f7fb",
   "metadata": {},
   "source": [
    "## Case of No period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41a8da",
   "metadata": {},
   "source": [
    "# Define parameters for lightcurve simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "times  = np.arange(0, 1000)\n",
    "dt = np.diff(times)[0]\n",
    "\n",
    "mean = 100\n",
    "\n",
    "#A = (mean * 0.1) ** 2 # variance of the lorentzian\n",
    "#Q = 80\n",
    "variance_drw = (mean * 0.1) ** 2  # variance of the DRW (bending powerlaw)\n",
    "w_bend = 2 * np.pi / 20 # angular frequency of the DRW or Bending Powerlaw\n",
    "\n",
    "# define the PSD model\n",
    "psd_model = BendingPowerlaw(variance_drw, w_bend) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659159b",
   "metadata": {},
   "source": [
    "# Simulate lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7adf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simulator object\n",
    "simulator = Simulator(psd_model, times, np.ones(len(times)) * dt, mean, pdf=\"Gaussian\")\n",
    "# simulate noiseless count rates from the PSD, make the initial lightcurve 2 times as long as the original times\n",
    "countrates = simulator.generate_lightcurve(extension_factor = 2)\n",
    "# add (Poisson) noise\n",
    "noisy_countrates, dy = simulator.add_noise(countrates)\n",
    "\n",
    "input_lc = GappyLightcurve(times, noisy_countrates, dy, exposures=dt)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.errorbar(times, noisy_countrates, yerr=dy)\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Rates (ct/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198f612",
   "metadata": {},
   "source": [
    "# Define null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e01205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null \n",
    "bounds_drw = dict(log_a=(-10, 50), log_c=(-10, 10))\n",
    "# you can use RealTerm from celerite or DampedRandomWalk from models.celerite_models\n",
    "null_kernel = celerite.terms.RealTerm(log_a=np.log(variance_drw), log_c=np.log(w_bend), bounds=bounds_drw)\n",
    "null_model = GPModelling(input_lc, null_kernel)\n",
    "print(\"Deriving posteriors for null model\")\n",
    "null_model.derive_posteriors(max_steps=50000, fit=True, cores=cpus)\n",
    "\n",
    "corner_fig = corner.corner(null_model.mcmc_samples, labels=null_model.gp.get_parameter_names(), title_fmt='.1f',\n",
    "                            quantiles=[0.16, 0.5, 0.84], show_titles=True, truths=[np.log(variance_drw), np.log(w_bend)],\n",
    "                            title_kwargs={\"fontsize\": 18}, max_n_ticks=3, labelpad=0.08,\n",
    "                            levels=(1 - np.exp(-0.5), 1 - np.exp(-0.5 * 2 ** 2))) # plots 1 and 2 sigma levels\n",
    "\n",
    "autocorr = null_model.autocorr\n",
    "fig = plt.figure()\n",
    "n = np.arange(1, len(autocorr) + 1)\n",
    "plt.plot(n, autocorr, \"-o\")\n",
    "plt.ylabel(\"Mean $\\\\tau$\")\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.savefig(\"autocorr.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6989bf",
   "metadata": {},
   "source": [
    "# Define alternative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f846ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 10 # period of the QPO\n",
    "w = 2 * np.pi / P\n",
    "# Define starting parameters\n",
    "log_variance_qpo = np.log(variance_drw)\n",
    "Q = 80 # coherence\n",
    "log_c = np.log(0.5 * w/Q)\n",
    "log_d = np.log(w)\n",
    "print(f\"log variance of the QPO: {log_variance_qpo:.2f}, log_c: {log_c:.2f}, log omega: {log_d:.2f}\")\n",
    "\n",
    "bounds_qpo = dict(log_a=(-10, 50), log_c=(-10, 10), log_d=(-5, 5))\n",
    "# You can also use Lorentzian from models.celerite_models (which is defined in terms of variance, Q and omega)\n",
    "alternative_kernel = celerite.terms.ComplexTerm(log_a=log_variance_qpo, log_c=log_c, log_d=log_d, bounds=bounds_qpo) \\\n",
    "     + celerite.terms.RealTerm(log_a=np.log(variance_drw), log_c=np.log(w_bend), bounds=bounds_drw)\n",
    "\n",
    "alternative_model = GPModelling(input_lc, alternative_kernel)\n",
    "print(\"Deriving posteriors for alternative model\")\n",
    "alternative_model.derive_posteriors(max_steps=50000, fit=True, cores=cpus)\n",
    "\n",
    "autocorr = alternative_model.autocorr\n",
    "fig = plt.figure()\n",
    "n = np.arange(1, len(autocorr) + 1)\n",
    "plt.plot(n, autocorr, \"-o\")\n",
    "plt.ylabel(\"Mean $\\\\tau$\")\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.savefig(\"autocorr.png\", dpi=100)\n",
    "\n",
    "corner_fig = corner.corner(alternative_model.mcmc_samples, labels=alternative_model.gp.get_parameter_names(), \n",
    "                           title_fmt='.1f',\n",
    "                            quantiles=[0.16, 0.5, 0.84], show_titles=True,\n",
    "                            title_kwargs={\"fontsize\": 18}, max_n_ticks=3, labelpad=0.08,\n",
    "                            levels=(1 - np.exp(-0.5), 1 - np.exp(-0.5 * 2 ** 2))) # plots 1 and 2 sigma levels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267bf517",
   "metadata": {},
   "source": [
    "# Generate lightcurves from null hypothesis posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsims = 100 # typically 10,000\n",
    "lcs = null_model.generate_from_posteriors(Nsims, cpus=cpus)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ae133",
   "metadata": {},
   "source": [
    "# Fit the lightcurves with both null and alternative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods_null = []\n",
    "likelihoods_alt = []\n",
    "\n",
    "for i, lc in enumerate(lcs):\n",
    "    print(\"Processing lightcurve %d/%d\" % (i + 1, len(lcs)), end=\"\\r\")\n",
    "    \n",
    "    # Run a small MCMC to make sure we find the global maximum of the likelihood\n",
    "    # ideally we'd probably want to run more samples\n",
    "    null_modelling = GPModelling(lc, null_kernel)\n",
    "    null_modelling.derive_posteriors(fit=True, cores=cpus, walkers=2 * cpus, max_steps=500, progress=False)\n",
    "    likelihoods_null.append(null_modelling.max_loglikelihodd)\n",
    "    alternative_modelling = GPModelling(lc, alternative_kernel)                         \n",
    "    alternative_modelling.derive_posteriors(fit=True, cores=cpus, walkers=2 * cpus, max_steps=500, \n",
    "                                            progress=False)\n",
    "    likelihoods_alt.append(alternative_modelling.max_loglikelihodd)\n",
    "    \n",
    "                                                                              \n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60745467",
   "metadata": {},
   "source": [
    "# Calculate T_LRT distribution and compare with the observed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "T_dist = -2 * (np.array(likelihoods_null) - np.array(likelihoods_alt))\n",
    "print(T_dist)\n",
    "plt.hist(T_dist, bins=10)\n",
    "T_obs = -2 * (null_model.max_loglikehood - alternative_model.max_loglikehood)\n",
    "print(\"Observed LRT_stat: %.3f\" % T_obs)\n",
    "perc = percentileofscore(T_dist, T_obs)\n",
    "print(\"p-value: %.4f\" % (1 - perc / 100))\n",
    "plt.axvline(T_obs, label=\"%.2f%%\" % perc, ls=\"--\", color=\"black\")\n",
    "\n",
    "sigmas = [95, 99.7]\n",
    "colors= [\"red\", \"green\"]\n",
    "for i, sigma in enumerate(sigmas):\n",
    "    plt.axvline(np.percentile(T_dist, sigma), ls=\"--\", color=colors[i])\n",
    "plt.legend()\n",
    "#plt.axvline(np.percentile(T_dist, 99.97), color=\"green\")\n",
    "plt.xlabel(\"$T_\\\\mathrm{LRT}$\")\n",
    "\n",
    "#plt.savefig(\"LRT_statistic.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade28540",
   "metadata": {},
   "source": [
    "We see the p-value to reject the null hypothesis is fairly low, indicating there is no signal in this data, as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a812146f",
   "metadata": {},
   "source": [
    "# Case with Period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e6037",
   "metadata": {},
   "source": [
    "# Simulate lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "times  = np.arange(0, 500)\n",
    "dt = np.diff(times)[0]\n",
    "\n",
    "mean = 100\n",
    "\n",
    "P = 10 # period of the QPO\n",
    "w_qpo = 2 * np.pi / P\n",
    "w_bend = 2 * np.pi / 20 # angular frequency of the DRW or Bending Powerlaw\n",
    "# Define starting parameters\n",
    "variance_drw = (mean * 0.1) ** 2  # variance of the DRW (bending powerlaw)\n",
    "variance_qpo = variance_drw # let's assume same variance for the QPO and the DRW\n",
    "\n",
    "Q = 80 # coherence\n",
    "\n",
    "psd_model = Lorentzian(variance_qpo, Q, w_qpo) + BendingPowerlaw(variance_drw, w_bend) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Simulator(psd_model, times, np.ones(len(times)) * dt, mean, pdf=\"Gaussian\", max_iter=500)\n",
    "\n",
    "rates = simulator.generate_lightcurve()\n",
    "noisy_rates, dy = simulator.add_noise(rates)\n",
    "\n",
    "input_lc = GappyLightcurve(times, noisy_rates, dy, exposures=dt)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.errorbar(times, noisy_rates, yerr=dy)\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Rates (ct/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa145a",
   "metadata": {},
   "source": [
    "# Define null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bounds_drw = dict(log_a=(-10, 50), log_c=(-10, 10))\n",
    "# null \n",
    "null_kernel = celerite.terms.RealTerm(log_a=np.log(variance_drw), log_c=np.log(w_bend), bounds=bounds_drw)\n",
    "null_model = GPModelling(input_lc, null_kernel)\n",
    "print(\"Deriving posteriors for null model\")\n",
    "null_model.derive_posteriors(max_steps=50000, fit=True, cores=cpus)\n",
    "\n",
    "corner_fig = corner.corner(null_model.mcmc_samples, labels=null_model.gp.get_parameter_names(), title_fmt='.1f',\n",
    "                            quantiles=[0.16, 0.5, 0.84], show_titles=True,\n",
    "                            title_kwargs={\"fontsize\": 18}, max_n_ticks=3, labelpad=0.08,\n",
    "                            levels=(1 - np.exp(-0.5), 1 - np.exp(-0.5 * 2 ** 2))) # plots 1 and 2 sigma levels\n",
    "\n",
    "autocorr = null_model.autocorr\n",
    "fig = plt.figure()\n",
    "n = np.arange(1, len(autocorr) + 1)\n",
    "plt.plot(n, autocorr, \"-o\")\n",
    "plt.ylabel(\"Mean $\\\\tau$\")\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.savefig(\"autocorr.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5563ef",
   "metadata": {},
   "source": [
    "# Define alternative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_c = np.log(0.5 * w_qpo/Q)\n",
    "log_d = np.log(w_qpo)\n",
    "bounds_qpo = dict(log_a=(-10, 50), log_c=(-10, 10), log_d=(-5, 5))\n",
    "# again you may use the Lorentzian from models.celerite_models\n",
    "alternative_kernel = celerite.terms.ComplexTerm(log_a=np.log(variance_qpo), log_c=log_c, \n",
    "                                                log_d=np.log(w_bend), bounds=bounds_qpo) \\\n",
    "     + celerite.terms.RealTerm(log_a=np.log(variance_drw), log_c=np.log(w_bend), bounds=bounds_bend)\n",
    "\n",
    "\n",
    "alternative_model = GPModelling(input_lc, alternative_kernel)\n",
    "print(\"Deriving posteriors for alternative model\")\n",
    "alternative_model.derive_posteriors(max_steps=50000, fit=True, cores=cpus)\n",
    "\n",
    "autocorr = alternative_model.autocorr\n",
    "fig = plt.figure()\n",
    "n = np.arange(1, len(autocorr) + 1)\n",
    "plt.plot(n, autocorr, \"-o\")\n",
    "plt.ylabel(\"Mean $\\\\tau$\")\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.savefig(\"autocorr.png\", dpi=100)\n",
    "\n",
    "corner_fig = corner.corner(alternative_model.mcmc_samples, labels=alternative_model.gp.get_parameter_names(), title_fmt='.1f',\n",
    "                            quantiles=[0.16, 0.5, 0.84], show_titles=True, \n",
    "                           truths=[np.log(variance_qpo), log_c, log_d, np.log(variance_drw), np.log(w_bend)],\n",
    "                            title_kwargs={\"fontsize\": 18}, max_n_ticks=3, labelpad=0.08,\n",
    "                            levels=(1 - np.exp(-0.5), 1 - np.exp(-0.5 * 2 ** 2))) # plots 1 and 2 sigma levels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e3da7",
   "metadata": {},
   "source": [
    "# Generate lightcurves with null hypothesis posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a99f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsims = 100 # typically 10,000\n",
    "lcs = null_model.generate_from_posteriors(Nsims, cpus=cpus)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941e9f8",
   "metadata": {},
   "source": [
    "# Fit the lightcurves with both null and alternative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ed115",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods_null = []\n",
    "likelihoods_alt = []\n",
    "\n",
    "for i, lc in enumerate(lcs):\n",
    "    print(\"Processing lightcurve %d/%d\" % (i + 1, len(lcs)), end=\"\\r\")\n",
    "    #fig = plt.figure()\n",
    "    #plt.errorbar(lc.times, lc.y, lc.dy)\n",
    "    #plt.xlabel(\"Time (days)\")\n",
    "    #plt.ylabel(\"Rate (ct/s)\")\n",
    "    #plt.savefig(\"%d.png\" % i, dpi=100)\n",
    "    \n",
    "    # Run a small MCMC to make sure we find the global maximum of the likelihood\n",
    "    # ideally we'd probably want to run more samples\n",
    "    null_modelling = GPModelling(lc, null_kernel)\n",
    "    null_modelling.derive_posteriors(fit=True, cores=cpus, walkers=2 * cpus, max_steps=500, progress=False)\n",
    "    likelihoods_null.append(null_modelling.max_loglikelihood)\n",
    "    alternative_modelling = GPModelling(lc, alternative_kernel)                         \n",
    "    alternative_modelling.derive_posteriors(fit=True, cores=cpus, walkers=2 * cpus, max_steps=500, \n",
    "                                            progress=False)\n",
    "    likelihoods_alt.append(alternative_modelling.max_loglikelihood)\n",
    "    \n",
    "                                                                              \n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76277a4d",
   "metadata": {},
   "source": [
    "# Calculate T_LRT distribution and compare with the observed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f01215",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "T_dist = -2 * (np.array(likelihoods_null) - np.array(likelihoods_alt))\n",
    "print(T_dist)\n",
    "plt.hist(T_dist, bins=10)\n",
    "T_obs = -2 * (null_model.max_loglikelihood - alternative_model.max_loglikelihood)\n",
    "print(\"Observed LRT_stat: %.3f\" % T_obs)\n",
    "perc = percentileofscore(T_dist, T_obs)\n",
    "print(\"p-value: %.4f\" % (1 - perc / 100))\n",
    "plt.axvline(T_obs, label=\"%.2f%%\" % perc, ls=\"--\", color=\"black\")\n",
    "\n",
    "sigmas = [95, 99.7]\n",
    "colors= [\"red\", \"green\"]\n",
    "for i, sigma in enumerate(sigmas):\n",
    "    plt.axvline(np.percentile(T_dist, sigma), ls=\"--\", color=colors[i])\n",
    "plt.legend()\n",
    "#plt.axvline(np.percentile(T_dist, 99.97), color=\"green\")\n",
    "plt.xlabel(\"$T_\\\\mathrm{LRT}$\")\n",
    "\n",
    "#plt.savefig(\"LRT_statistic.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df7dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
